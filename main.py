import os
from dotenv import load_dotenv
import json
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

load_dotenv()

# Inicializar o modelo de linguagem
llm = ChatOpenAI(model_name="gpt-3.5-turbo",max_tokens = 200)

system_message = SystemMessage(
    "Você é um guia de estudos que ajuda estudantes a aprenderem novos tópicos. \n\n"
    "Seu trabalho é guiar os estudante para que ele consiga entender"
    "tópico escolhido sem receber respostas prontas da sua parte. \n\n"
    "Evite conversar sobre assuntos paralelos ao tópico escolhido. Se o estudante "
    "não fornecer um tópico inicialmente, seu primeiro trabalho será solicitar um "
    "tópico até que o estudante o informe. \n\n"
    "Você pode ser amigável, descolado e tratar o estudante como adolescente. Queremos "
    "evitar a fadiga de um estudo rígido e mantê-lo engajado no que estiver "
    "estudando. \n\n"
    "As próximas mensagens serão de um estudante. "
)

humano_message = HumanMessage(
    "Olá, tudo bem, meu nome é Ricardo"
)

# Inicializa a lista de mensagens com system_message e humano_message
messages = [system_message, humano_message]

response = llm.invoke(messages)
print(f"{'AI':-^80}")
# Imprime a resposta simples
print(response.content)

# Também podemos fazer um loop infinito e montar um histórico de conversa
# artificialmente. Mas isso não é necessário quando usamos LangGraph.

# Adiciona a resposta do modelo em messages
messages.append(response)
while True:
    # Pega a mensagem do usuário
    print(f"{'Human':-^80}")
    user_input = input("Digite sua mensagem: ")
    human_message = HumanMessage(user_input)

    # Qualquer uma dessas palavras termina o loop
    if user_input.lower() in ["exit", "quit", "bye", "q"]:
        break

    # Adiciona a mensagem do usuário em messages
    messages.append(human_message)

    # Manda as mensagens com o histórico de volta para o modelo
    response = llm.invoke(messages)

    # Exibe a mensagem do modelo
    print(f"{'AI':-^80}")
    print(response.content)
    print()

    # Adiciona a resposta do modelo em messages
    messages.append(response)

# Isso é só para vermos como ficou nosso histórico de conversas
print()
print(f"{'Histórico':-^80}")
print(*[f"{m.type.upper()}\n{m.content}\n\n" for m in messages], sep="", end="")
print()